{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hyperparameters Tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOHc1fGaPCbXAoS9d8I2GJf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedEssam19/Graduation-Project/blob/Hyperparameter-Tuning/Hyperparameters_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "id": "Yi4D0on9Lej0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1_bAXzdCRBjoPSkO_MrQ_FRoM_-npeJll"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpSyTqttLfBS",
        "outputId": "6b2f9146-ab7a-4eac-99f1-0d438d5e04a3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_bAXzdCRBjoPSkO_MrQ_FRoM_-npeJll\n",
            "To: /content/data.zip\n",
            "100% 2.05G/2.05G [00:40<00:00, 50.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/data.zip' -d '/content/'"
      ],
      "metadata": {
        "id": "8Ooan6d4Lgmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLKBVgCzpYbh"
      },
      "outputs": [],
      "source": [
        "pip install \"ray[tune]\" torch torchvision pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim  \n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import os\n",
        "from torchvision.io import decode_jpeg\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets,models\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image"
      ],
      "metadata": {
        "id": "0O9vcHPMpc5p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "El5YyoW2LlpD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CreateDataset(Dataset):\n",
        "    def __init__(self, df,transform=False):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):   \n",
        "        img_path = self.df.iloc[index, 0]\n",
        "        image = read_image(img_path) / 255.0\n",
        "        label = self.df.iloc[index, 1]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "ntwXIWSNLotK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "read_image('data/Camera 1/train/c0/1589.jpg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFirmDQoc5oI",
        "outputId": "54053e28-4b90-4c87-a5d0-065473fb9997"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 63,  65,  67,  ..., 234, 234, 234],\n",
              "         [ 63,  65,  67,  ..., 234, 234, 234],\n",
              "         [ 63,  65,  67,  ..., 234, 234, 234],\n",
              "         ...,\n",
              "         [ 54,  56,  58,  ...,   0,   0,   0],\n",
              "         [ 54,  56,  58,  ...,   0,   0,   0],\n",
              "         [ 54,  56,  58,  ...,   0,   0,   0]],\n",
              "\n",
              "        [[ 64,  66,  68,  ..., 235, 235, 235],\n",
              "         [ 64,  66,  68,  ..., 235, 235, 235],\n",
              "         [ 64,  66,  68,  ..., 235, 235, 235],\n",
              "         ...,\n",
              "         [ 27,  29,  31,  ...,   2,   2,   2],\n",
              "         [ 27,  29,  31,  ...,   2,   2,   2],\n",
              "         [ 27,  29,  31,  ...,   2,   2,   2]],\n",
              "\n",
              "        [[ 46,  48,  50,  ..., 229, 229, 229],\n",
              "         [ 46,  48,  50,  ..., 229, 229, 229],\n",
              "         [ 46,  48,  50,  ..., 229, 229, 229],\n",
              "         ...,\n",
              "         [  0,   2,   4,  ...,   1,   1,   1],\n",
              "         [  0,   2,   4,  ...,   1,   1,   1],\n",
              "         [  0,   2,   4,  ...,   1,   1,   1]]], dtype=torch.uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformers = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_df = pd.read_csv(\"data/train.csv\")\n",
        "val_df = pd.read_csv(\"data/val.csv\")\n",
        "test_df = pd.read_csv(\"data/test.csv\")\n",
        "\n",
        "train_dataset=CreateDataset(train_df, transformers)\n",
        "test_dataset=CreateDataset(test_df, transformers)\n",
        "val_dataset=CreateDataset(val_df, transformers)"
      ],
      "metadata": {
        "id": "T7hRfvrIOymm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=32,shuffle=False)\n",
        "val_dataloader = DataLoader(dataset=val_dataset, batch_size=32,shuffle=False)"
      ],
      "metadata": {
        "id": "iKJatIMPMmsC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "import torchmetrics\n",
        "from torch import nn\n",
        "\n",
        "class Model(pl.LightningModule):\n",
        "   \n",
        "    def __init__(self, output_units, config, freeze_base=False):\n",
        "        super().__init__()\n",
        "        self.base_model = torchvision.models.resnet50(pretrained=True)\n",
        "\n",
        "        freezing_layers = [\n",
        "            self.base_model.conv1,\n",
        "            self.base_model.bn1,\n",
        "            self.base_model.layer1,\n",
        "            self.base_model.layer2,\n",
        "        ]\n",
        "        for layer in freezing_layers:\n",
        "          for param in layer.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.base_model.fc = torch.nn.Linear(in_features=self.base_model.fc.in_features, out_features=500)\n",
        "        self.clf = torch.nn.Linear(in_features=500, out_features=output_units)\n",
        "        self.lr = config[\"lr\"]\n",
        "        self.dropout = torch.nn.Dropout(p=config[\"dropout\"])\n",
        "        #self.batch_size = config[\"batch_size\"]\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.train_acc = torchmetrics.Accuracy()\n",
        "        self.val_acc = torchmetrics.Accuracy()\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        features = self.base_model(input_data)\n",
        "        features = self.dropout(features)\n",
        "        return self.clf(features)\n",
        "\n",
        "    def cross_entropy_loss(self, logits, labels):\n",
        "        return F.nll_loss(logits, labels)\n",
        "\n",
        "    def accuracy(self, logits, labels):\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        correct = (predicted == labels).sum().item()\n",
        "        accuracy = correct / len(labels)\n",
        "        return torch.tensor(accuracy)\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        input_data, targets = batch\n",
        "        preds = self(input_data)\n",
        "        loss = self.criterion(preds, targets)\n",
        "        self.log('train_loss', loss)\n",
        "        self.train_acc(preds, targets)\n",
        "        self.log('train_acc', self.train_acc, on_step=True, on_epoch=False, prog_bar=True)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        input_data, targets = batch\n",
        "        preds = self(input_data)\n",
        "        loss = self.criterion(preds, targets)\n",
        "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
        "        self.val_acc(preds, targets)\n",
        "        self.log('val_acc', self.val_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
        "        avg_acc = torch.stack([x[\"val_accuracy\"] for x in outputs]).mean()\n",
        "        self.log(\"ptl/val_loss\", avg_loss)\n",
        "        self.log(\"ptl/val_accuracy\", avg_acc)\n",
        "\n",
        "    def test_step(self, batch, batch_nb):\n",
        "        self.validation_step(batch, batch_nb)\n",
        "        \n",
        "    def predict_step(self, batch, batch_nb):\n",
        "        input_data, targets = batch\n",
        "        preds = self(input_data)\n",
        "        return torch.argmax(preds, dim=1)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        return optimizer"
      ],
      "metadata": {
        "id": "POuznoADpgsC"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(config):\n",
        "    model = Model(NUM_CLASSES,config)\n",
        "    trainer = pl.Trainer(max_epochs=10, show_progress_bar=False)\n",
        "\n",
        "    trainer.fit(model,train_dataloader,val_dataloader)"
      ],
      "metadata": {
        "id": "SUKadRZtgZKe"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\n",
        "from ray.tune.integration.pytorch_lightning import TuneReportCallback, \\\n",
        "    TuneReportCheckpointCallback"
      ],
      "metadata": {
        "id": "mwqJSGrgplC1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_tune(config, num_epochs=10, num_gpus=1):\n",
        "    model = Model(NUM_CLASSES,config)\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=num_epochs,\n",
        "        # If fractional GPUs passed in, convert to int.\n",
        "        gpus=math.ceil(num_gpus),\n",
        "        logger=TensorBoardLogger(\n",
        "            save_dir=tune.get_trial_dir(), name=\"\", version=\".\"),\n",
        "        progress_bar_refresh_rate=0,\n",
        "        callbacks=[\n",
        "            TuneReportCallback(\n",
        "                {\n",
        "                    \"loss\": \"ptl/val_loss\",\n",
        "                    \"mean_accuracy\": \"ptl/val_accuracy\"\n",
        "                },\n",
        "                on=\"validation_end\")\n",
        "        ])\n",
        "\n",
        "    trainer.fit(model,train_dataloader,val_dataloader)"
      ],
      "metadata": {
        "id": "9fOmxwltpoy0"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_model_asha(num_samples=10, num_epochs=10, gpus_per_trial=1):\n",
        "    config = {\n",
        "      \"dropout\": tune.choice([0.1, 0.15, 0.2, 0.25, 0.3]),\n",
        "      \"lr\": tune.loguniform(1e-4, 1e-1)\n",
        "      #\"batch_size\": tune.choice([32, 64, 128]),\n",
        "    }\n",
        "\n",
        "    scheduler = ASHAScheduler(\n",
        "        max_t=num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "\n",
        "    reporter = CLIReporter(\n",
        "        parameter_columns=[\"dropout\", \"lr\"],\n",
        "        metric_columns=[\"loss\", \"mean_accuracy\", \"training_iteration\"])\n",
        "\n",
        "    train_fn_with_parameters = tune.with_parameters(train_model_tune,\n",
        "                                                    num_epochs=num_epochs,\n",
        "                                                    num_gpus=gpus_per_trial,\n",
        "                                                    )\n",
        "    resources_per_trial = {\"cpu\": 1, \"gpu\": gpus_per_trial}\n",
        "\n",
        "    analysis = tune.run(train_fn_with_parameters,\n",
        "        resources_per_trial=resources_per_trial,\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter,\n",
        "        name=\"tune_model_asha\")\n",
        "\n",
        "    best_result = analysis.best_config\n",
        "    print(\"Best hyperparameters found were: \", best_result)"
      ],
      "metadata": {
        "id": "FmV5AoOFrEC1"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tune_model_asha()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAwAu65bsjgz",
        "outputId": "d7d3745d-b74d-4b6f-9faf-e40e81bdfcba"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2022-02-14 23:10:06 (running for 00:00:00.29)\n",
            "Memory usage on this node: 2.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/6.53 GiB heap, 0.0/3.27 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/tune_model_asha\n",
            "Number of trials: 10/10 (10 PENDING)\n",
            "+------------------------------+----------+-------+-----------+-------------+\n",
            "| Trial name                   | status   | loc   |   dropout |          lr |\n",
            "|------------------------------+----------+-------+-----------+-------------|\n",
            "| train_model_tune_3fae1_00000 | PENDING  |       |      0.1  | 0.0143832   |\n",
            "| train_model_tune_3fae1_00001 | PENDING  |       |      0.2  | 0.000219493 |\n",
            "| train_model_tune_3fae1_00002 | PENDING  |       |      0.1  | 0.0124832   |\n",
            "| train_model_tune_3fae1_00003 | PENDING  |       |      0.25 | 0.000480705 |\n",
            "| train_model_tune_3fae1_00004 | PENDING  |       |      0.2  | 0.00302288  |\n",
            "| train_model_tune_3fae1_00005 | PENDING  |       |      0.15 | 0.0117861   |\n",
            "| train_model_tune_3fae1_00006 | PENDING  |       |      0.1  | 0.00419683  |\n",
            "| train_model_tune_3fae1_00007 | PENDING  |       |      0.1  | 0.0229284   |\n",
            "| train_model_tune_3fae1_00008 | PENDING  |       |      0.1  | 0.000693146 |\n",
            "| train_model_tune_3fae1_00009 | PENDING  |       |      0.15 | 0.0399795   |\n",
            "+------------------------------+----------+-------+-----------+-------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2022-02-14 23:10:12 (running for 00:00:05.38)\n",
            "Memory usage on this node: 3.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/6.53 GiB heap, 0.0/3.27 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/tune_model_asha\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+------------------------------+----------+-----------------+-----------+-------------+\n",
            "| Trial name                   | status   | loc             |   dropout |          lr |\n",
            "|------------------------------+----------+-----------------+-----------+-------------|\n",
            "| train_model_tune_3fae1_00000 | RUNNING  | 172.28.0.2:1272 |      0.1  | 0.0143832   |\n",
            "| train_model_tune_3fae1_00001 | PENDING  |                 |      0.2  | 0.000219493 |\n",
            "| train_model_tune_3fae1_00002 | PENDING  |                 |      0.1  | 0.0124832   |\n",
            "| train_model_tune_3fae1_00003 | PENDING  |                 |      0.25 | 0.000480705 |\n",
            "| train_model_tune_3fae1_00004 | PENDING  |                 |      0.2  | 0.00302288  |\n",
            "| train_model_tune_3fae1_00005 | PENDING  |                 |      0.15 | 0.0117861   |\n",
            "| train_model_tune_3fae1_00006 | PENDING  |                 |      0.1  | 0.00419683  |\n",
            "| train_model_tune_3fae1_00007 | PENDING  |                 |      0.1  | 0.0229284   |\n",
            "| train_model_tune_3fae1_00008 | PENDING  |                 |      0.1  | 0.000693146 |\n",
            "| train_model_tune_3fae1_00009 | PENDING  |                 |      0.15 | 0.0399795   |\n",
            "+------------------------------+----------+-----------------+-----------+-------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   | Name       | Type             | Params\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m ------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m 0 | base_model | ResNet           | 24.5 M\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m 1 | clf        | Linear           | 5.0 K \n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m 2 | dropout    | Dropout          | 0     \n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m 3 | criterion  | CrossEntropyLoss | 0     \n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m 4 | train_acc  | Accuracy         | 0     \n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m 5 | val_acc    | Accuracy         | 0     \n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m ------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m 23.1 M    Trainable params\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m 1.4 M     Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m 24.5 M    Total params\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m 98.150    Total estimated model params size (MB)\n",
            "2022-02-14 23:10:17,674\tERROR trial_runner.py:927 -- Trial train_model_tune_3fae1_00000: Error processing event.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/trial_runner.py\", line 893, in _process_trial\n",
            "    results = self.trial_executor.fetch_result(trial)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/ray_trial_executor.py\", line 707, in fetch_result\n",
            "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/worker.py\", line 1733, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=1272, ip=172.28.0.2, repr=train_model_tune)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/trainable.py\", line 315, in train\n",
            "    result = self.step()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 381, in step\n",
            "    self._report_thread_runner_error(block=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 532, in _report_thread_runner_error\n",
            "    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n",
            "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
            "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=1272, ip=172.28.0.2, repr=train_model_tune)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 262, in run\n",
            "    self._entrypoint()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 331, in entrypoint\n",
            "    self._status_reporter.get_checkpoint())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 600, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/utils/trainable.py\", line 353, in _inner\n",
            "    inner(config, checkpoint_dir=None)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/utils/trainable.py\", line 344, in inner\n",
            "    trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-43-975d18cdad85>\", line 20, in train_model_tune\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 741, in fit\n",
            "    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 685, in _call_and_handle_interrupt\n",
            "    return trainer_fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 777, in _fit_impl\n",
            "    self._run(model, ckpt_path=ckpt_path)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1199, in _run\n",
            "    self._dispatch()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1279, in _dispatch\n",
            "    self.training_type_plugin.start_training(self)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 202, in start_training\n",
            "    self._results = trainer.run_stage()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1289, in run_stage\n",
            "    return self._run_train()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1311, in _run_train\n",
            "    self._run_sanity_check(self.lightning_module)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1375, in _run_sanity_check\n",
            "    self._evaluation_loop.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 145, in run\n",
            "    self.advance(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 110, in advance\n",
            "    dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 140, in run\n",
            "    self.on_run_start(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 86, in on_run_start\n",
            "    self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_progress.current.ready)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/utilities.py\", line 121, in _update_dataloader_iter\n",
            "    dataloader_iter = enumerate(data_fetcher, batch_idx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\", line 199, in __iter__\n",
            "    self.prefetching(self.prefetch_batches)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\", line 258, in prefetching\n",
            "    self._fetch_next_batch()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\", line 300, in _fetch_next_batch\n",
            "    batch = next(self.dataloader_iter)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 561, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"<ipython-input-7-e55461d65152>\", line 11, in __getitem__\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py\", line 222, in read_image\n",
            "    data = read_file(path)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py\", line 42, in read_file\n",
            "    data = torch.ops.image.read_file(path)\n",
            "RuntimeError: [Errno 2] No such file or directory: 'data/Camera 1/train/c0/1920.jpg'\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m 2022-02-14 23:10:17,660\tERROR function_runner.py:268 -- Runner Thread raised error.\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 262, in run\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self._entrypoint()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 331, in entrypoint\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self._status_reporter.get_checkpoint())\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     return method(self, *_args, **_kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 600, in _trainable_func\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     output = fn()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/utils/trainable.py\", line 353, in _inner\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     inner(config, checkpoint_dir=None)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/utils/trainable.py\", line 344, in inner\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     trainable(config, **fn_kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"<ipython-input-43-975d18cdad85>\", line 20, in train_model_tune\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 741, in fit\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 685, in _call_and_handle_interrupt\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     return trainer_fn(*args, **kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 777, in _fit_impl\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1199, in _run\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self._dispatch()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1279, in _dispatch\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self.training_type_plugin.start_training(self)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 202, in start_training\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self._results = trainer.run_stage()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1289, in run_stage\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     return self._run_train()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1311, in _run_train\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self._run_sanity_check(self.lightning_module)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1375, in _run_sanity_check\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self._evaluation_loop.run()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 145, in run\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self.advance(*args, **kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 110, in advance\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 140, in run\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self.on_run_start(*args, **kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 86, in on_run_start\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_progress.current.ready)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/utilities.py\", line 121, in _update_dataloader_iter\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     dataloader_iter = enumerate(data_fetcher, batch_idx)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\", line 199, in __iter__\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self.prefetching(self.prefetch_batches)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\", line 258, in prefetching\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self._fetch_next_batch()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\", line 300, in _fetch_next_batch\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     batch = next(self.dataloader_iter)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     data = self._next_data()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 561, in _next_data\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"<ipython-input-7-e55461d65152>\", line 11, in __getitem__\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py\", line 222, in read_image\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     data = read_file(path)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py\", line 42, in read_file\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     data = torch.ops.image.read_file(path)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m RuntimeError: [Errno 2] No such file or directory: 'data/Camera 1/train/c0/1920.jpg'\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m Exception in thread Thread-2:\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self.run()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 281, in run\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     raise e\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 262, in run\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self._entrypoint()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 331, in entrypoint\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self._status_reporter.get_checkpoint())\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     return method(self, *_args, **_kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 600, in _trainable_func\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     output = fn()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/utils/trainable.py\", line 353, in _inner\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     inner(config, checkpoint_dir=None)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/utils/trainable.py\", line 344, in inner\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     trainable(config, **fn_kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"<ipython-input-43-975d18cdad85>\", line 20, in train_model_tune\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 741, in fit\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 685, in _call_and_handle_interrupt\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     return trainer_fn(*args, **kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 777, in _fit_impl\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1199, in _run\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self._dispatch()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1279, in _dispatch\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self.training_type_plugin.start_training(self)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 202, in start_training\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self._results = trainer.run_stage()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1289, in run_stage\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     return self._run_train()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1311, in _run_train\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self._run_sanity_check(self.lightning_module)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1375, in _run_sanity_check\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self._evaluation_loop.run()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 145, in run\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self.advance(*args, **kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 110, in advance\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 140, in run\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self.on_run_start(*args, **kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 86, in on_run_start\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_progress.current.ready)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/utilities.py\", line 121, in _update_dataloader_iter\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     dataloader_iter = enumerate(data_fetcher, batch_idx)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\", line 199, in __iter__\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self.prefetching(self.prefetch_batches)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\", line 258, in prefetching\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     self._fetch_next_batch()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\", line 300, in _fetch_next_batch\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     batch = next(self.dataloader_iter)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     data = self._next_data()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 561, in _next_data\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"<ipython-input-7-e55461d65152>\", line 11, in __getitem__\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py\", line 222, in read_image\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     data = read_file(path)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py\", line 42, in read_file\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m     data = torch.ops.image.read_file(path)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m RuntimeError: [Errno 2] No such file or directory: 'data/Camera 1/train/c0/1920.jpg'\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1272)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_model_tune_3fae1_00000:\n",
            "  date: 2022-02-14_23-10-10\n",
            "  experiment_id: 126e3badbad547dca60ebb0480f9f1f1\n",
            "  hostname: a5da6c3c5d00\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1272\n",
            "  timestamp: 1644880210\n",
            "  trial_id: 3fae1_00000\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2022-02-14 23:10:17 (running for 00:00:11.00)\n",
            "Memory usage on this node: 4.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/6.53 GiB heap, 0.0/3.27 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/tune_model_asha\n",
            "Number of trials: 10/10 (1 ERROR, 9 PENDING)\n",
            "+------------------------------+----------+-----------------+-----------+-------------+\n",
            "| Trial name                   | status   | loc             |   dropout |          lr |\n",
            "|------------------------------+----------+-----------------+-----------+-------------|\n",
            "| train_model_tune_3fae1_00001 | PENDING  |                 |      0.2  | 0.000219493 |\n",
            "| train_model_tune_3fae1_00002 | PENDING  |                 |      0.1  | 0.0124832   |\n",
            "| train_model_tune_3fae1_00003 | PENDING  |                 |      0.25 | 0.000480705 |\n",
            "| train_model_tune_3fae1_00004 | PENDING  |                 |      0.2  | 0.00302288  |\n",
            "| train_model_tune_3fae1_00005 | PENDING  |                 |      0.15 | 0.0117861   |\n",
            "| train_model_tune_3fae1_00006 | PENDING  |                 |      0.1  | 0.00419683  |\n",
            "| train_model_tune_3fae1_00007 | PENDING  |                 |      0.1  | 0.0229284   |\n",
            "| train_model_tune_3fae1_00008 | PENDING  |                 |      0.1  | 0.000693146 |\n",
            "| train_model_tune_3fae1_00009 | PENDING  |                 |      0.15 | 0.0399795   |\n",
            "| train_model_tune_3fae1_00000 | ERROR    | 172.28.0.2:1272 |      0.1  | 0.0143832   |\n",
            "+------------------------------+----------+-----------------+-----------+-------------+\n",
            "Number of errored trials: 1\n",
            "+------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                   |   # failures | error file                                                                                                             |\n",
            "|------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_model_tune_3fae1_00000 |            1 | /root/ray_results/tune_model_asha/train_model_tune_3fae1_00000_0_dropout=0.1,lr=0.014383_2022-02-14_23-10-06/error.txt |\n",
            "+------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2022-02-14 23:10:23 (running for 00:00:16.97)\n",
            "Memory usage on this node: 3.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/6.53 GiB heap, 0.0/3.27 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/tune_model_asha\n",
            "Number of trials: 10/10 (1 ERROR, 8 PENDING, 1 RUNNING)\n",
            "+------------------------------+----------+-----------------+-----------+-------------+\n",
            "| Trial name                   | status   | loc             |   dropout |          lr |\n",
            "|------------------------------+----------+-----------------+-----------+-------------|\n",
            "| train_model_tune_3fae1_00001 | RUNNING  | 172.28.0.2:1271 |      0.2  | 0.000219493 |\n",
            "| train_model_tune_3fae1_00002 | PENDING  |                 |      0.1  | 0.0124832   |\n",
            "| train_model_tune_3fae1_00003 | PENDING  |                 |      0.25 | 0.000480705 |\n",
            "| train_model_tune_3fae1_00004 | PENDING  |                 |      0.2  | 0.00302288  |\n",
            "| train_model_tune_3fae1_00005 | PENDING  |                 |      0.15 | 0.0117861   |\n",
            "| train_model_tune_3fae1_00006 | PENDING  |                 |      0.1  | 0.00419683  |\n",
            "| train_model_tune_3fae1_00007 | PENDING  |                 |      0.1  | 0.0229284   |\n",
            "| train_model_tune_3fae1_00008 | PENDING  |                 |      0.1  | 0.000693146 |\n",
            "| train_model_tune_3fae1_00009 | PENDING  |                 |      0.15 | 0.0399795   |\n",
            "| train_model_tune_3fae1_00000 | ERROR    | 172.28.0.2:1272 |      0.1  | 0.0143832   |\n",
            "+------------------------------+----------+-----------------+-----------+-------------+\n",
            "Number of errored trials: 1\n",
            "+------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                   |   # failures | error file                                                                                                             |\n",
            "|------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_model_tune_3fae1_00000 |            1 | /root/ray_results/tune_model_asha/train_model_tune_3fae1_00000_0_dropout=0.1,lr=0.014383_2022-02-14_23-10-06/error.txt |\n",
            "+------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "== Status ==\n",
            "Current time: 2022-02-14 23:10:28 (running for 00:00:22.06)\n",
            "Memory usage on this node: 4.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/6.53 GiB heap, 0.0/3.27 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/tune_model_asha\n",
            "Number of trials: 10/10 (1 ERROR, 8 PENDING, 1 RUNNING)\n",
            "+------------------------------+----------+-----------------+-----------+-------------+\n",
            "| Trial name                   | status   | loc             |   dropout |          lr |\n",
            "|------------------------------+----------+-----------------+-----------+-------------|\n",
            "| train_model_tune_3fae1_00001 | RUNNING  | 172.28.0.2:1271 |      0.2  | 0.000219493 |\n",
            "| train_model_tune_3fae1_00002 | PENDING  |                 |      0.1  | 0.0124832   |\n",
            "| train_model_tune_3fae1_00003 | PENDING  |                 |      0.25 | 0.000480705 |\n",
            "| train_model_tune_3fae1_00004 | PENDING  |                 |      0.2  | 0.00302288  |\n",
            "| train_model_tune_3fae1_00005 | PENDING  |                 |      0.15 | 0.0117861   |\n",
            "| train_model_tune_3fae1_00006 | PENDING  |                 |      0.1  | 0.00419683  |\n",
            "| train_model_tune_3fae1_00007 | PENDING  |                 |      0.1  | 0.0229284   |\n",
            "| train_model_tune_3fae1_00008 | PENDING  |                 |      0.1  | 0.000693146 |\n",
            "| train_model_tune_3fae1_00009 | PENDING  |                 |      0.15 | 0.0399795   |\n",
            "| train_model_tune_3fae1_00000 | ERROR    | 172.28.0.2:1272 |      0.1  | 0.0143832   |\n",
            "+------------------------------+----------+-----------------+-----------+-------------+\n",
            "Number of errored trials: 1\n",
            "+------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                   |   # failures | error file                                                                                                             |\n",
            "|------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_model_tune_3fae1_00000 |            1 | /root/ray_results/tune_model_asha/train_model_tune_3fae1_00000_0_dropout=0.1,lr=0.014383_2022-02-14_23-10-06/error.txt |\n",
            "+------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m \n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   | Name       | Type             | Params\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m ------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m 0 | base_model | ResNet           | 24.5 M\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m 1 | clf        | Linear           | 5.0 K \n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m 2 | dropout    | Dropout          | 0     \n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m 3 | criterion  | CrossEntropyLoss | 0     \n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m 4 | train_acc  | Accuracy         | 0     \n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m 5 | val_acc    | Accuracy         | 0     \n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m ------------------------------------------------\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m 23.1 M    Trainable params\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m 1.4 M     Non-trainable params\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m 24.5 M    Total params\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m 98.150    Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m 2022-02-14 23:10:29,237\tERROR function_runner.py:268 -- Runner Thread raised error.\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 262, in run\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self._entrypoint()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 331, in entrypoint\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self._status_reporter.get_checkpoint())\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     return method(self, *_args, **_kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 600, in _trainable_func\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     output = fn()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/utils/trainable.py\", line 353, in _inner\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     inner(config, checkpoint_dir=None)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/utils/trainable.py\", line 344, in inner\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     trainable(config, **fn_kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"<ipython-input-43-975d18cdad85>\", line 20, in train_model_tune\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 741, in fit\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 685, in _call_and_handle_interrupt\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     return trainer_fn(*args, **kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 777, in _fit_impl\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1199, in _run\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self._dispatch()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1279, in _dispatch\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self.training_type_plugin.start_training(self)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 202, in start_training\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self._results = trainer.run_stage()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1289, in run_stage\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     return self._run_train()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1311, in _run_train\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self._run_sanity_check(self.lightning_module)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1375, in _run_sanity_check\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self._evaluation_loop.run()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 145, in run\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self.advance(*args, **kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 110, in advance\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 140, in run\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self.on_run_start(*args, **kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 86, in on_run_start\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_progress.current.ready)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/utilities.py\", line 121, in _update_dataloader_iter\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     dataloader_iter = enumerate(data_fetcher, batch_idx)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\", line 199, in __iter__\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self.prefetching(self.prefetch_batches)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\", line 258, in prefetching\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self._fetch_next_batch()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\", line 300, in _fetch_next_batch\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     batch = next(self.dataloader_iter)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     data = self._next_data()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 561, in _next_data\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"<ipython-input-7-e55461d65152>\", line 11, in __getitem__\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py\", line 222, in read_image\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     data = read_file(path)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py\", line 42, in read_file\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     data = torch.ops.image.read_file(path)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m RuntimeError: [Errno 2] No such file or directory: 'data/Camera 1/train/c0/1920.jpg'\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m Exception in thread Thread-2:\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self.run()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 281, in run\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     raise e\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 262, in run\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self._entrypoint()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 331, in entrypoint\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self._status_reporter.get_checkpoint())\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     return method(self, *_args, **_kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 600, in _trainable_func\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     output = fn()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/utils/trainable.py\", line 353, in _inner\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     inner(config, checkpoint_dir=None)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/utils/trainable.py\", line 344, in inner\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     trainable(config, **fn_kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"<ipython-input-43-975d18cdad85>\", line 20, in train_model_tune\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 741, in fit\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 685, in _call_and_handle_interrupt\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     return trainer_fn(*args, **kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 777, in _fit_impl\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1199, in _run\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self._dispatch()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1279, in _dispatch\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self.training_type_plugin.start_training(self)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 202, in start_training\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self._results = trainer.run_stage()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1289, in run_stage\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     return self._run_train()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1311, in _run_train\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self._run_sanity_check(self.lightning_module)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1375, in _run_sanity_check\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self._evaluation_loop.run()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 145, in run\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self.advance(*args, **kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 110, in advance\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 140, in run\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self.on_run_start(*args, **kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 86, in on_run_start\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_progress.current.ready)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/utilities.py\", line 121, in _update_dataloader_iter\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     dataloader_iter = enumerate(data_fetcher, batch_idx)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\", line 199, in __iter__\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self.prefetching(self.prefetch_batches)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\", line 258, in prefetching\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     self._fetch_next_batch()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\", line 300, in _fetch_next_batch\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     batch = next(self.dataloader_iter)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     data = self._next_data()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 561, in _next_data\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"<ipython-input-7-e55461d65152>\", line 11, in __getitem__\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py\", line 222, in read_image\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     data = read_file(path)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py\", line 42, in read_file\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m     data = torch.ops.image.read_file(path)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m RuntimeError: [Errno 2] No such file or directory: 'data/Camera 1/train/c0/1920.jpg'\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1271)\u001b[0m \n",
            "2022-02-14 23:10:29,346\tERROR trial_runner.py:927 -- Trial train_model_tune_3fae1_00001: Error processing event.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/trial_runner.py\", line 893, in _process_trial\n",
            "    results = self.trial_executor.fetch_result(trial)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/ray_trial_executor.py\", line 707, in fetch_result\n",
            "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/worker.py\", line 1733, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=1271, ip=172.28.0.2, repr=train_model_tune)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/trainable.py\", line 315, in train\n",
            "    result = self.step()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 381, in step\n",
            "    self._report_thread_runner_error(block=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 532, in _report_thread_runner_error\n",
            "    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n",
            "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
            "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=1271, ip=172.28.0.2, repr=train_model_tune)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 262, in run\n",
            "    self._entrypoint()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 331, in entrypoint\n",
            "    self._status_reporter.get_checkpoint())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 600, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/utils/trainable.py\", line 353, in _inner\n",
            "    inner(config, checkpoint_dir=None)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/utils/trainable.py\", line 344, in inner\n",
            "    trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-43-975d18cdad85>\", line 20, in train_model_tune\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 741, in fit\n",
            "    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 685, in _call_and_handle_interrupt\n",
            "    return trainer_fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 777, in _fit_impl\n",
            "    self._run(model, ckpt_path=ckpt_path)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1199, in _run\n",
            "    self._dispatch()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1279, in _dispatch\n",
            "    self.training_type_plugin.start_training(self)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 202, in start_training\n",
            "    self._results = trainer.run_stage()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1289, in run_stage\n",
            "    return self._run_train()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1311, in _run_train\n",
            "    self._run_sanity_check(self.lightning_module)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1375, in _run_sanity_check\n",
            "    self._evaluation_loop.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 145, in run\n",
            "    self.advance(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 110, in advance\n",
            "    dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 140, in run\n",
            "    self.on_run_start(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 86, in on_run_start\n",
            "    self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_progress.current.ready)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/utilities.py\", line 121, in _update_dataloader_iter\n",
            "    dataloader_iter = enumerate(data_fetcher, batch_idx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\", line 199, in __iter__\n",
            "    self.prefetching(self.prefetch_batches)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\", line 258, in prefetching\n",
            "    self._fetch_next_batch()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\", line 300, in _fetch_next_batch\n",
            "    batch = next(self.dataloader_iter)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 561, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"<ipython-input-7-e55461d65152>\", line 11, in __getitem__\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py\", line 222, in read_image\n",
            "    data = read_file(path)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py\", line 42, in read_file\n",
            "    data = torch.ops.image.read_file(path)\n",
            "RuntimeError: [Errno 2] No such file or directory: 'data/Camera 1/train/c0/1920.jpg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_model_tune_3fae1_00001:\n",
            "  date: 2022-02-14_23-10-22\n",
            "  experiment_id: 91df609a0b8549cfaa09557ff2a900fa\n",
            "  hostname: a5da6c3c5d00\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1271\n",
            "  timestamp: 1644880222\n",
            "  trial_id: 3fae1_00001\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2022-02-14 23:10:34 (running for 00:00:28.07)\n",
            "Memory usage on this node: 2.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/6.53 GiB heap, 0.0/3.27 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/tune_model_asha\n",
            "Number of trials: 10/10 (2 ERROR, 7 PENDING, 1 RUNNING)\n",
            "+------------------------------+----------+-----------------+-----------+-------------+\n",
            "| Trial name                   | status   | loc             |   dropout |          lr |\n",
            "|------------------------------+----------+-----------------+-----------+-------------|\n",
            "| train_model_tune_3fae1_00002 | RUNNING  | 172.28.0.2:1345 |      0.1  | 0.0124832   |\n",
            "| train_model_tune_3fae1_00003 | PENDING  |                 |      0.25 | 0.000480705 |\n",
            "| train_model_tune_3fae1_00004 | PENDING  |                 |      0.2  | 0.00302288  |\n",
            "| train_model_tune_3fae1_00005 | PENDING  |                 |      0.15 | 0.0117861   |\n",
            "| train_model_tune_3fae1_00006 | PENDING  |                 |      0.1  | 0.00419683  |\n",
            "| train_model_tune_3fae1_00007 | PENDING  |                 |      0.1  | 0.0229284   |\n",
            "| train_model_tune_3fae1_00008 | PENDING  |                 |      0.1  | 0.000693146 |\n",
            "| train_model_tune_3fae1_00009 | PENDING  |                 |      0.15 | 0.0399795   |\n",
            "| train_model_tune_3fae1_00000 | ERROR    | 172.28.0.2:1272 |      0.1  | 0.0143832   |\n",
            "| train_model_tune_3fae1_00001 | ERROR    | 172.28.0.2:1271 |      0.2  | 0.000219493 |\n",
            "+------------------------------+----------+-----------------+-----------+-------------+\n",
            "Number of errored trials: 2\n",
            "+------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                   |   # failures | error file                                                                                                               |\n",
            "|------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_model_tune_3fae1_00000 |            1 | /root/ray_results/tune_model_asha/train_model_tune_3fae1_00000_0_dropout=0.1,lr=0.014383_2022-02-14_23-10-06/error.txt   |\n",
            "| train_model_tune_3fae1_00001 |            1 | /root/ray_results/tune_model_asha/train_model_tune_3fae1_00001_1_dropout=0.2,lr=0.00021949_2022-02-14_23-10-08/error.txt |\n",
            "+------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m   f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:276: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m   \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "2022-02-14 23:10:36,840\tWARNING tune.py:593 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m 2022-02-14 23:10:36,878\tERROR worker.py:432 -- SystemExit was raised from the worker.\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m   File \"python/ray/_raylet.pyx\", line 770, in ray._raylet.task_execution_handler\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m   File \"python/ray/_raylet.pyx\", line 591, in ray._raylet.execute_task\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m   File \"python/ray/_raylet.pyx\", line 629, in ray._raylet.execute_task\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m   File \"python/ray/_raylet.pyx\", line 636, in ray._raylet.execute_task\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.execute_task\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m   File \"python/ray/_raylet.pyx\", line 589, in ray._raylet.execute_task.function_executor\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/_private/function_manager.py\", line 639, in actor_method_executor\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m     return method(self, *_args, **_kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/trainable.py\", line 315, in train\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m     result = self.step()\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m     return method(self, *_args, **_kwargs)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 364, in step\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m     block=True, timeout=RESULT_FETCH_TIMEOUT)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m   File \"/usr/lib/python3.7/queue.py\", line 179, in get\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m     self.not_empty.wait(remaining)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m   File \"/usr/lib/python3.7/threading.py\", line 300, in wait\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/worker.py\", line 429, in sigterm_handler\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m     sys.exit(1)\n",
            "\u001b[2m\u001b[36m(train_model_tune pid=1345)\u001b[0m SystemExit: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2022-02-14 23:10:36 (running for 00:00:30.17)\n",
            "Memory usage on this node: 3.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/6.53 GiB heap, 0.0/3.27 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/tune_model_asha\n",
            "Number of trials: 10/10 (2 ERROR, 7 PENDING, 1 RUNNING)\n",
            "+------------------------------+----------+-----------------+-----------+-------------+\n",
            "| Trial name                   | status   | loc             |   dropout |          lr |\n",
            "|------------------------------+----------+-----------------+-----------+-------------|\n",
            "| train_model_tune_3fae1_00002 | RUNNING  | 172.28.0.2:1345 |      0.1  | 0.0124832   |\n",
            "| train_model_tune_3fae1_00003 | PENDING  |                 |      0.25 | 0.000480705 |\n",
            "| train_model_tune_3fae1_00004 | PENDING  |                 |      0.2  | 0.00302288  |\n",
            "| train_model_tune_3fae1_00005 | PENDING  |                 |      0.15 | 0.0117861   |\n",
            "| train_model_tune_3fae1_00006 | PENDING  |                 |      0.1  | 0.00419683  |\n",
            "| train_model_tune_3fae1_00007 | PENDING  |                 |      0.1  | 0.0229284   |\n",
            "| train_model_tune_3fae1_00008 | PENDING  |                 |      0.1  | 0.000693146 |\n",
            "| train_model_tune_3fae1_00009 | PENDING  |                 |      0.15 | 0.0399795   |\n",
            "| train_model_tune_3fae1_00000 | ERROR    | 172.28.0.2:1272 |      0.1  | 0.0143832   |\n",
            "| train_model_tune_3fae1_00001 | ERROR    | 172.28.0.2:1271 |      0.2  | 0.000219493 |\n",
            "+------------------------------+----------+-----------------+-----------+-------------+\n",
            "Number of errored trials: 2\n",
            "+------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                   |   # failures | error file                                                                                                               |\n",
            "|------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_model_tune_3fae1_00000 |            1 | /root/ray_results/tune_model_asha/train_model_tune_3fae1_00000_0_dropout=0.1,lr=0.014383_2022-02-14_23-10-06/error.txt   |\n",
            "| train_model_tune_3fae1_00001 |            1 | /root/ray_results/tune_model_asha/train_model_tune_3fae1_00001_1_dropout=0.2,lr=0.00021949_2022-02-14_23-10-08/error.txt |\n",
            "+------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-02-14 23:10:37,101\tERROR tune.py:632 -- Trials did not complete: [train_model_tune_3fae1_00000, train_model_tune_3fae1_00001, train_model_tune_3fae1_00002, train_model_tune_3fae1_00003, train_model_tune_3fae1_00004, train_model_tune_3fae1_00005, train_model_tune_3fae1_00006, train_model_tune_3fae1_00007, train_model_tune_3fae1_00008, train_model_tune_3fae1_00009]\n",
            "2022-02-14 23:10:37,103\tINFO tune.py:636 -- Total run time: 30.45 seconds (30.16 seconds for the tuning loop).\n",
            "2022-02-14 23:10:37,109\tWARNING tune.py:641 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n",
            "2022-02-14 23:10:37,122\tWARNING experiment_analysis.py:533 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters found were:  None\n"
          ]
        }
      ]
    }
  ]
}